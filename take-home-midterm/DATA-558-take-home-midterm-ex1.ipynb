{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DATA 558: Take Home Midterm\n",
    "\n",
    "Geoffrey Li\n",
    "\n",
    "May 19, 2019"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "import datetime"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1: Load, Process Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Consider the Spam dataset from The Elements of Statistical Learning. Standardize the data, if you have not done so already. Be sure to use the training and test splits from the website."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "spam = pd.read_csv('./spam.data', sep='\\s+', header=None)\n",
    "test_indicator = pd.read_csv('./spam.traintest.txt', header=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create our X matrix with the predictors and y vector with the response\n",
    "X = np.asarray(spam)[:, 0:-1]\n",
    "y = np.asarray(spam)[:, -1]*2 - 1 # Convert to +/- 1\n",
    "test_indicator = np.array(test_indicator).T[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Divide the data into train, test sets\n",
    "X_train = X[test_indicator == 0, :]\n",
    "X_test = X[test_indicator == 1, :]\n",
    "y_train = y[test_indicator == 0][:, np.newaxis]\n",
    "y_test = y[test_indicator == 1][:, np.newaxis]\n",
    "\n",
    "y = y[:, np.newaxis]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import preprocessing\n",
    "\n",
    "# Standardize the data\n",
    "scaler = preprocessing.StandardScaler().fit(X_train)\n",
    "X_train = scaler.transform(X_train)\n",
    "X_test = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Keep track of the number of samples and dimension of each sample\n",
    "n_train = len(y_train)\n",
    "n_test = len(y_test)\n",
    "d = np.size(X, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2: Define Gradient Descent Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\ell_2^{2}$-regularized binary logistic regression with $\\rho$-logistic loss\n",
    "\n",
    "Compute the gradient ∇F(β) of F."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\\nabla F(\\beta) = \\frac{1}{n} \\sum_{i=1}^{n} -y_i x_i^{T} \\frac{exp(-\\rho y_ix_i^{T}\\beta)}{1+exp(-\\rho y_ix_i^{T}\\beta)} +2 \\lambda \\beta $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def computegrad(x, y, beta, lamb, rho):\n",
    "    p = np.identity(len(x)) - np.diag(1/(1+np.exp(np.multiply(-rho*y, x@beta))).reshape(1, -1)[0])\n",
    "    return -1/len(x) * x.T @ p @ y + 2*lamb*beta\n",
    "\n",
    "\n",
    "def computeobj(x, y, beta, lamb, rho):\n",
    "    return 1/(len(x)*rho)*np.sum(np.log(1+np.exp(np.multiply(-rho*y, x@beta)))) + lamb*np.sum(beta**2)\n",
    "\n",
    "\n",
    "def backtracking(curr_beta, lamb, x, y, rho, eta_t=1, alpha=0.5, gamma=0.5, max_iter=100):\n",
    "    grad_curr_beta = computegrad(x, y, curr_beta, lamb, rho)  # Gradient at current beta\n",
    "    norm_grad_curr_beta = np.sqrt(np.sum(grad_curr_beta ** 2))  # Norm of the gradient at current beta\n",
    "    found_eta_t = False\n",
    "    i = 0  # Iteration counter\n",
    "\n",
    "    while (found_eta_t is False and i < max_iter):\n",
    "        if (computeobj(x, y, curr_beta - eta_t * grad_curr_beta, lamb, rho) <\n",
    "                computeobj(x, y, curr_beta, lamb, rho) - alpha * eta_t * norm_grad_curr_beta ** 2):\n",
    "            found_eta_t = True\n",
    "        elif i == max_iter - 1:\n",
    "            raise ('Maximum number of iterations of backtracking reached')\n",
    "        else:\n",
    "            eta_t *= gamma\n",
    "            i += 1\n",
    "\n",
    "    return eta_t\n",
    "\n",
    "\n",
    "def initstepsize(x, lamb):\n",
    "    return 1/(max(np.linalg.eigh(1/len(x)*x.T@x)[0]) + lamb)\n",
    "\n",
    "\n",
    "def graddescent(beta_init, lamb, x, y, rho, ss_init, targ_acc):\n",
    "    beta_values = list()\n",
    "    beta_values.append(beta_init)\n",
    "\n",
    "    grad_beta = computegrad(x, y, beta_init, lamb, rho)\n",
    "    norm_grad_beta = np.sqrt(np.sum(grad_beta ** 2))\n",
    "    tuned_step_size = ss_init\n",
    "\n",
    "    t = 0\n",
    "\n",
    "    while norm_grad_beta > targ_acc:\n",
    "        tuned_step_size = backtracking(beta_values[t], lamb, x, y, rho, \n",
    "                                       eta_t=tuned_step_size, alpha=0.5, gamma=0.8, max_iter=100)\n",
    "        beta_values.append(beta_values[t] - tuned_step_size * grad_beta)\n",
    "\n",
    "        t += 1\n",
    "\n",
    "        grad_beta = computegrad(x, y, beta_values[t], lamb, rho)\n",
    "        norm_grad_beta = np.sqrt(np.sum(grad_beta ** 2))\n",
    "\n",
    "    return beta_values\n",
    "\n",
    "\n",
    "def fastgradalgo(beta_init, lamb, x, y, rho, theta_init, ss_init, targ_acc):\n",
    "    beta_values = list()\n",
    "    beta_values.append(beta_init)\n",
    "    theta = theta_init\n",
    "\n",
    "    grad_theta = computegrad(x, y, theta, lamb, rho)\n",
    "\n",
    "    grad_beta = computegrad(x, y, beta_init, lamb, rho)\n",
    "    norm_grad_beta = np.sqrt(np.sum(grad_beta ** 2))\n",
    "\n",
    "    tuned_step_size = ss_init\n",
    "\n",
    "    t = 0\n",
    "\n",
    "    while norm_grad_beta > targ_acc:\n",
    "        grad_theta = computegrad(x, y, theta, lamb, rho)\n",
    "\n",
    "        tuned_step_size = backtracking(beta_values[t], lamb, x, y, rho, \n",
    "                                       eta_t=tuned_step_size, alpha=0.5, gamma=0.8, max_iter=100)\n",
    "\n",
    "        beta_values.append(theta - tuned_step_size * grad_theta)\n",
    "        theta = beta_values[t + 1] + (t / (t + 3)) * (beta_values[t + 1] - beta_values[t])\n",
    "\n",
    "        t += 1\n",
    "\n",
    "        grad_beta = computegrad(x, y, beta_values[t], lamb, rho)\n",
    "        norm_grad_beta = np.sqrt(np.sum(grad_beta ** 2))\n",
    "\n",
    "    return beta_values\n",
    "\n",
    "\n",
    "def misclassificationerror(y, x, b):\n",
    "    return 1 - np.mean(\n",
    "        np.fromiter(map(lambda p: 1 if p >= 0.5 else -1, 1/(1+np.exp(-x@b))), \n",
    "                    dtype=np.int).reshape(-1,1) == y)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Write a function myrhologistic that implements the accelerated gradient algorithm to train the l2-regularized binary logistic regression with ρ-logistic loss. The function takes as input the initial step-size for the backtracking rule, the ε for the stopping criterion based on the norm of the gradient of the objective, and the value of ρ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def myrhologistic(x_train, y_train, init_eta, target_accuracy, rho, lamb):\n",
    "    init_beta = np.zeros(x_train.shape[1])[:, np.newaxis]\n",
    "    init_theta = np.zeros(x_train.shape[1])[:, np.newaxis]\n",
    "    \n",
    "    # Run fast gradient and train classifier\n",
    "    beta_opt = fastgradalgo(init_beta, lamb, x_train, y_train, rho, init_theta, init_eta, target_accuracy)\n",
    "    beta_opt_T = beta_opt[len(beta_opt)-1]\n",
    "    \n",
    "    return beta_opt_T"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3: Define Cross-Validation Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Write a function crossval that implements leave-one-out cross-validation and hold-out cross- validation. You may either write a function that implements each variant separately depend- ing on the case, or write a general cross-validation function that can be instantiated in each case."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def crossval(x, y, testsetsize, params, targ_acc, rho):\n",
    "    # testsetsize denotes percentage of training data to be split as test data (e.g. 0.20 is 80/20 split)\n",
    "    # testsetsize = 1 indicates leave-one-out CV\n",
    "    \n",
    "    n = len(x)\n",
    "    cv_results = dict()\n",
    "    errors = list()\n",
    "    \n",
    "    if testsetsize == 1: #leave-one-out CV\n",
    "        for l in params:\n",
    "            print('Start CV for Lambda = '+str(l))\n",
    "            eta_init = initstepsize(x, l)\n",
    "            errors = list()\n",
    "            \n",
    "            for i in range(n):\n",
    "                cv_x_test = x[i,:]\n",
    "                cv_y_test = y[i,:]\n",
    "                cv_x_train = np.delete(x, i, 0)\n",
    "                cv_y_train = np.delete(y, i, 0)\n",
    "\n",
    "                beta_T = myrhologistic(cv_x_train, cv_y_train, eta_init, targ_acc, rho, l)\n",
    "                errors.append(misclassificationerror(cv_y_test, cv_x_test, beta_T))\n",
    "                print('LOO CV Iteration '+str(i)+' complete.')\n",
    "            \n",
    "            cv_results[l] = np.mean(errors)\n",
    "    elif testsetsize > 0 and testsetsize < 1:\n",
    "        testset_idx = random.sample(range(n), int(n*testsetsize)) # Generate random test set indices\n",
    "        cv_x_test = x[testset_idx,:]\n",
    "        cv_y_test = y[testset_idx,:]\n",
    "        cv_x_train = np.delete(x, testset_idx, 0)\n",
    "        cv_y_train = np.delete(y, testset_idx, 0)\n",
    "        \n",
    "        for l in params:\n",
    "            eta_init = initstepsize(x, l)\n",
    "            \n",
    "            beta_T = myrhologistic(cv_x_train, cv_y_train, eta_init, targ_acc, rho, l)\n",
    "            cv_results[l] = misclassificationerror(cv_y_test, cv_x_test, beta_T)\n",
    "            \n",
    "    else:\n",
    "        raise('Invalid test set size.')\n",
    "    \n",
    "    return min(cv_results, key=cv_results.get), cv_results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.4 Train Model with $\\lambda = 1$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train your l2-regularized binary logistic regression with ρ-logistic loss with ρ = 2 and ε = 10−3 on the the Spam dataset for the λ = 1. Report your misclassification error for this value of λ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-05-17 02:16:59.017872\n",
      "2019-05-17 02:17:04.298353\n"
     ]
    }
   ],
   "source": [
    "print(datetime.datetime.now())\n",
    "\n",
    "# Initialize parameters\n",
    "lambduh = 1\n",
    "target_accuracy = 10**-3\n",
    "r = 2\n",
    "eta_init = initstepsize(X_train, lambduh)\n",
    "\n",
    "# Train the Myrhologistic Classifier\n",
    "beta_T_naive = myrhologistic(X_train, y_train, eta_init, target_accuracy, r, lambduh)\n",
    "\n",
    "print(datetime.datetime.now())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.10677083333333337"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Compute Misclassification Error\n",
    "misclassificationerror(y_test, X_test, beta_T_naive)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.5: Train Model: Optimal $\\lambda$ using 80/20 Hold-Out CV"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Find the optimal value of λ using hold-out cross-validation with a 80%/20% split for the training set/testing set. Report your misclassification errors for the two values of λ found."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-05-17 02:17:04.318644\n",
      "2019-05-17 02:20:44.203575\n"
     ]
    }
   ],
   "source": [
    "print(datetime.datetime.now())\n",
    "\n",
    "# Initialize parameters\n",
    "target_accuracy = 10**-3\n",
    "r = 2\n",
    "test_set_fraction = 0.20\n",
    "lambs = [10**i for i in range(-4, 3)]\n",
    "\n",
    "# Run CV to find optimal lambda\n",
    "opt_lamb, cv_res_holdout = crossval(X_train, y_train, test_set_fraction, lambs, target_accuracy, r)\n",
    "eta_init = initstepsize(X_train, opt_lamb)\n",
    "\n",
    "# Train the Myrhologistic Classifier\n",
    "beta_T_cv_holdout = myrhologistic(X_train, y_train, eta_init, target_accuracy, r, opt_lamb)\n",
    "\n",
    "\n",
    "\n",
    "print(datetime.datetime.now())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.08268229166666663"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Compute Misclassification Error\n",
    "misclassificationerror(y_test, X_test, beta_T_cv_holdout)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Misclassification improved from the $\\lambda = 1$ run, as expected."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.6: Train Model: Optimal $\\lambda$ using Leave-One-Out CV"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Find the optimal value of λ using leave-one-out cross-validation. Report your misclassification errors for the two values of λ found."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(datetime.datetime.now())\n",
    "\n",
    "# Initialize parameters\n",
    "target_accuracy = 10**-2\n",
    "r = 2\n",
    "test_set_fraction = 1\n",
    "lambs = [10**i for i in range(2, -2, -1)]\n",
    "\n",
    "# Run CV to find optimal lambda\n",
    "opt_lamb, cv_res_loo = crossval(X_train, y_train, test_set_fraction, lambs, target_accuracy, r)\n",
    "eta_init = initstepsize(X_train, opt_lamb)\n",
    "\n",
    "# Train the Myrhologistic Classifier\n",
    "beta_T_cv_loo = myrhologistic(X_train, y_train, eta_init, target_accuracy, r, opt_lamb)\n",
    "\n",
    "\n",
    "print(datetime.datetime.now())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.08854166666666663"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Compute Misclassification Error\n",
    "misclassificationerror(y_test, X_test, beta_T_cv_loo)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "LOO CV is extremely computationally efficient, as we need to train the model n times for each $\\lambda$ value. Despite the extra CV time, the misclassification error isn't even better than 80/20 holdout CV. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
